# Optimization Plan v1.0

Target: Reduce cycles well below baseline 147,734 by exploiting VLIW packing and SIMD, while preserving correctness under the submission tests.

## Priorities

1) VLIW packing
- Batch multiple independent scalar slots into the same bundle respecting `SLOT_LIMITS` to reduce cycles without changing algorithm semantics.

2) Vectorization of inner loop
- Process `VLEN` elements of the batch per iteration:
  - Use `vload`/`vstore` for `values` and contiguous `indices` windows.
  - Maintain vector temporaries for `idx`, `val`, `node_val`.
  - Compute hash with `valu` ops; consider staging constants with `vbroadcast`.

3) Address generation
- Hoist base pointers; compute contiguous addresses once per `VLEN` chunk.
- Use `flow.add_imm` to bump scalar pointers per iteration when applicable.

4) Branchless next-index selection
- Use `vselect` to choose `+1` vs `+2` without divergent control flow.
- Bounds wrap with vector `<` and `vselect`.

5) Reduce debug costs during performance runs
- Keep `debug` slots only for trace mode; disable for performance measurements.

6) Hash stage fusion
- Where legal, group dependent `alu/valu` ops to maximize per-cycle parallel occupancy; ensure hazard-free scheduling by separating write-after-read in bundles.

## Implementation Sketch

- Extend `KernelBuilder.build` to pack multiple slots per engine per cycle up to limits.
- Add vector scratch regions: `idx_v`, `val_v`, `node_v`, `pred_v` (length = VLEN each).
- Build outer loops: rounds -> chunks of `VLEN` across batch.
- For each chunk:
  - `vload` `val_v` and `idx_v` from base + offset.
  - Gather `node_v` is non-contiguous; initial version uses scalar loads in parallel bundles. Later, precompute a small gather via staging.
  - Apply hash across `HASH_STAGES` with `valu` ops and per-stage constants via `vbroadcast`.
  - Compute parity vector, derive `+1` vs `+2`, update idx vector, wrap, `vstore` back.

## Validation

- Keep `pause` positions aligned with `reference_kernel2` yields.
- Use `tests/submission_tests.py` as the sole correctness oracle; never modify `tests/`.

## Risks / Mitigations

- Scratch pressure: size vectors carefully and reuse temporaries; spill only scalars when needed.
- Gather pattern for tree values is non-contiguous: retain scalar fallback or explore block-reordering only if allowed by semantics.

## Next Steps

- Implement VLIW packing helper with simple greedy bin-packing per bundle.
- Introduce vectorized path guarded by batch multiple of `VLEN`; fallback to scalar remainder loop.
- Add a perf flag to disable debug slots during cycle measurements.

